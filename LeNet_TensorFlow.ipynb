{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet-TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ac3rVjpDvnDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LeNet Tensorflow Implementation"
      ]
    },
    {
      "metadata": {
        "id": "MNFUGBz6vre9",
        "colab_type": "code",
        "outputId": "dad05ce3-cbb6-469d-8826-5ad0670bf44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test             = mnist.test.images, mnist.test.labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-931c722e6e3b>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5kg5y2bav5fC",
        "colab_type": "code",
        "outputId": "90503835-83cd-45b9-92ab-62998402ed02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "print(f\"Image Size: {X_train.shape[1:]}\")\n",
        "print(f\"Training Set Samples: {X_train.shape[0]}\")\n",
        "print(f\"Validation Set Samples: {X_validation.shape[0]}\")\n",
        "print(f\"Test Set Samples: {X_test.shape[0]}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Size: (28, 28, 1)\n",
            "Training Set Samples: 55000\n",
            "Validation Set Samples: 5000\n",
            "Test Set Samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s2Grl23zv8c-",
        "colab_type": "code",
        "outputId": "4d935586-2a7b-4dd5-bac1-9e25bc88355d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "index = random.randint(0, len(X_train))\n",
        "image = X_train[index].squeeze()\n",
        "\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "print(y_train[index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFcAAABYCAYAAACAnmu5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vrIvGPdExye9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_c3vULCyL47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE=0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lje3nbNlyOR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_weights(shape, mean=0, stddev=0.1):\n",
        "  initial_val = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n",
        "  return tf.Variable(initial_val, dtype=tf.float32)\n",
        "\n",
        "def initialize_biases(shape, bias_val=0):\n",
        "  initial_val = tf.constant(bias_val, shape=shape, dtype=tf.float32)\n",
        "  return tf.Variable(initial_val, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def LeNet(X):\n",
        "  with tf.name_scope(\"LeNet\"):\n",
        "    W_conv1 = initialize_weights(shape=(5,5,1,6))\n",
        "    b_conv1 = initialize_biases(shape=[6])\n",
        "    conv1 = tf.nn.conv2d(X, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
        "\n",
        "    h_conv1 = tf.nn.relu(conv1)\n",
        "    \n",
        "    pool1 = tf.nn.max_pool(h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    \n",
        "    W_conv2 = initialize_weights(shape=(5,5,6,16))\n",
        "    b_conv2 = initialize_biases(shape=[16])\n",
        "    conv2 = tf.nn.conv2d(pool1, W_conv2, strides=[1,1,1,1], padding='VALID')\n",
        "    \n",
        "    h_conv2 = tf.nn.relu(conv2)\n",
        "    \n",
        "    pool2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "\n",
        "    fc0 = tf.layers.flatten(pool2, name='fc0') \n",
        "    \n",
        "    W_fc1 = initialize_weights(shape=(400,120))\n",
        "    b_fc1 = initialize_biases(shape=[120])\n",
        "    fc1 = tf.matmul(fc0, W_fc1) + b_fc1\n",
        "    \n",
        "    h_fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    W_fc2 = initialize_weights(shape=(120,84))\n",
        "    b_fc2 = initialize_biases(shape=[84])\n",
        "    fc2 = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "    \n",
        "    h_fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    W_fc3 = initialize_weights(shape=(84,10))\n",
        "    b_fc3 = initialize_biases(shape=[10])\n",
        "    logits = tf.matmul(h_fc2, W_fc3) + b_fc3\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIXqN_079qNJ",
        "colab_type": "code",
        "outputId": "23945421-65e2-4064-aaca-afe7a96f6c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "def normalize_fixed(x, current_min, current_max, normed_min, normed_max):\n",
        "    x_normed = (x - current_min) / (current_max - current_min)\n",
        "    x_normed = x_normed * (normed_max - normed_min) + normed_min\n",
        "    return x_normed\n",
        "\n",
        "X = normalize_fixed(tf.placeholder(tf.float32, (None, 28, 28, 1)), 0, 255, -1, 1)\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).repeat(EPOCHS).batch(BATCH_SIZE)\n",
        "iterator = train_dataset.make_initializable_iterator()\n",
        "next_batch = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p5igMoNnFacp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = LeNet(X)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "  cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
        "  loss = tf.reduce_mean(cross_entropy)\n",
        "  \n",
        "with tf.name_scope(\"train\"): \n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
        "  training = optimizer.minimize(loss)\n",
        "  \n",
        "with tf.name_scope(\"eval\"):\n",
        "  accuracy = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, axis=1))[1]\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vXMm2eUrP10I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.group(      \n",
        "    tf.global_variables_initializer(),\n",
        "    tf.local_variables_initializer(),\n",
        "    iterator.initializer)\n",
        "  )\n",
        "  writer = tf.summary.FileWriter('./log', sess.graph)\n",
        "  \n",
        "  num_examples = len(X_train)\n",
        "  new_accuracy = 0\n",
        "  print(\"Training...\")\n",
        "  for i in range(EPOCHS):\n",
        "    print(\"EPOCH {}: \".format(i+1))\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "      end = offset + BATCH_SIZE\n",
        "      batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "      sess.run(training, feed_dict={X: batch_x, y: batch_y})\n",
        "      accuracy_ = sess.run(accuracy, {X: X_validation, y: y_validation})\n",
        "      print(\"Validation Accuracy = {:.3f}\".format(accuracy_))\n",
        "      \n",
        "    if accuracy_ > new_accuracy:\n",
        "      new_accuracy = accuracy_\n",
        "    elif accuracy_ < new_accuracy:\n",
        "      print(\"Early Stopping\")\n",
        "      break\n",
        "    saver.save(sess, './lenet.ckpt')\n",
        "    print(\"Model saved\")\n",
        "  saver.save(sess, './lenet_final.ckpt')\n",
        "  print('Final Model Saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWDFewzCyi4A",
        "colab_type": "code",
        "outputId": "74d30dba-fc1a-4293-b53b-386dc2266397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './lenet_final.ckpt')\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    accuracy_ = sess.run(accuracy, {X: X_test, y: y_test})\n",
        "    print(\"Test Accuracy = {:.3f}\".format(accuracy_))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./lenet_final.ckpt\n",
            "Test Accuracy = 0.989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHkm_gBUjLwo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}